{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLgk8cBMBQoK"
      },
      "source": [
        "### Welcome to the Motiverse AI 2025 Hackathon at IISc Bangalore! This document contains starter code that might be helpful as you tackle one or both of the problems.\n",
        "### Problem 1: Help Center Chatbot\n",
        "####Using the information available on Motiveâ€™s public website, construct a much more capable chatbot\n",
        "\n",
        "You are provided with a scraped and curated dataset of all the content from the motive public website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvQbcDP2r4Nf",
        "outputId": "ab62e3f5-12c7-4343-bd03-bc08eae9112c"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import boto3\n",
        "\n",
        "API_KEYS_FILE = \"api_keys.json\"\n",
        "\n",
        "def load_api_keys():\n",
        "    \"\"\"Load API keys from a JSON file.\"\"\"\n",
        "    if not os.path.exists(API_KEYS_FILE):\n",
        "        print(\"API keys file not found.\")\n",
        "        return None\n",
        "    \n",
        "    with open(API_KEYS_FILE, \"r\") as f:\n",
        "        keys = json.load(f)\n",
        "    return keys\n",
        "\n",
        "keys = load_api_keys()\n",
        "\n",
        "GROQ_AI_key = keys.get(\"GROQ_AI_key2\")\n",
        "os.environ['GROQ_API_KEY'] = GROQ_AI_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSWmsS5trF6X"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id = ''\n",
        "aws_secret_access_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load text data and save chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow_2-xz8rIOf"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "# Create an S3 client\n",
        "s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "def list_s3_bucket_contents(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists all objects in the specified S3 bucket.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The name of the S3 bucket\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # List objects in the bucket\n",
        "        response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
        "\n",
        "        if 'Contents' in response:\n",
        "            print(f\"Contents of bucket '{bucket_name}':\")\n",
        "            for obj in response['Contents']:\n",
        "                print(f\" - {obj['Key']} (Last Modified: {obj['LastModified']}, Size: {obj['Size']} bytes)\")\n",
        "        else:\n",
        "            print(f\"No objects found in bucket '{bucket_name}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqcQTTZzrIUL"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "def read_s3_file(bucket_name, key_name, aws_access_key_id, aws_secret_access_key):\n",
        "    \"\"\"\n",
        "    Reads a file from S3 and returns its content.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The S3 bucket name\n",
        "        key_name (str): The S3 object key\n",
        "        aws_access_key_id (str): AWS access key ID\n",
        "        aws_secret_access_key (str): AWS secret access key\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create S3 client\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key\n",
        "        )\n",
        "\n",
        "        # Create temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "            temp_path = temp_file.name\n",
        "\n",
        "        try:\n",
        "            # Download file from S3\n",
        "            s3_client.download_file(bucket_name, key_name, temp_path)\n",
        "\n",
        "            # Read the file content\n",
        "            with open(temp_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            return content\n",
        "\n",
        "        finally:\n",
        "            # Clean up temporary file\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file from S3: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def process_web_content(content):\n",
        "    \"\"\"\n",
        "    Processes the web content and creates a dictionary with page keys and data tags.\n",
        "\n",
        "    Args:\n",
        "        content (str): The content of the web_content.txt file\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with page keys and their corresponding data\n",
        "    \"\"\"\n",
        "    pages = {}\n",
        "    current_page = None\n",
        "    current_data = []\n",
        "\n",
        "    # Split content into lines\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Skip empty lines\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check if line starts with a page marker (e.g., \"PAGE:\" or similar)\n",
        "        if line.startswith('PAGE:') or line.startswith('URL:'):\n",
        "            # If we have a current page, save its data\n",
        "            if current_page:\n",
        "                pages[current_page] = {\n",
        "                    'data': '\\n'.join(current_data),\n",
        "                    'tags': extract_tags(current_data)\n",
        "                }\n",
        "\n",
        "            # Start new page\n",
        "            current_page = line\n",
        "            current_data = []\n",
        "        else:\n",
        "            # Add line to current page's data\n",
        "            current_data.append(line)\n",
        "\n",
        "    # Add the last page if exists\n",
        "    if current_page and current_data:\n",
        "        pages[current_page] = {\n",
        "            'data': '\\n'.join(current_data),\n",
        "            'tags': extract_tags(current_data)\n",
        "        }\n",
        "\n",
        "    return pages\n",
        "\n",
        "def extract_tags(data_lines):\n",
        "    \"\"\"\n",
        "    Extracts tags from the data lines.\n",
        "\n",
        "    Args:\n",
        "        data_lines (list): List of data lines for a page\n",
        "\n",
        "    Returns:\n",
        "        list: List of extracted tags\n",
        "    \"\"\"\n",
        "    tags = []\n",
        "    for line in data_lines:\n",
        "        # Look for tag markers (e.g., \"TAG:\", \"CATEGORY:\", etc.)\n",
        "        if 'TAG:' in line or 'CATEGORY:' in line:\n",
        "            tag = line.split(':', 1)[1].strip()\n",
        "            if tag:\n",
        "                tags.append(tag)\n",
        "    return tags\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ekA3NMvsTKl",
        "outputId": "c8601d69-fd1b-4084-fa49-c2881fe63bf9"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'motiverse-2025-data'\n",
        "key_name = 'web_content.txt'\n",
        "\n",
        "try:\n",
        "    # Read the file from S3\n",
        "    print(f\"Reading {key_name} from {bucket_name}...\")\n",
        "    content = read_s3_file(bucket_name, key_name, aws_access_key_id, aws_secret_access_key)\n",
        "\n",
        "    # Process the content\n",
        "    print(\"Processing content...\")\n",
        "    pages = process_web_content(content)\n",
        "\n",
        "    print(f\"Processed {len(pages)} pages.\")\n",
        "    print('Page keys: ', pages['URL: https://gomotive.com/content-library/report/motive-2023-annual-roi-report-2023-10/'].keys())\n",
        "\n",
        "    # Print sample of the processed data\n",
        "    print(\"\\nSample of processed data:\")\n",
        "    for page_key, page_data in list(pages.items())[:30]:  # Show first 2 pages as sample\n",
        "        print(f\"\\nPage: {page_key}\")\n",
        "        print(f\"Tags: {page_data['tags']}\")\n",
        "        print(f\"Data preview: {page_data['data']}...\")\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Chunk data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Bedrock client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key\n",
        ")\n",
        "\n",
        "def calculate_cosine_distances(sentences):\n",
        "    distances = []\n",
        "    for i in range(len(sentences) - 1):\n",
        "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
        "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
        "        \n",
        "        # Convert to cosine distance\n",
        "        distance = 1 - similarity\n",
        "\n",
        "        # Append cosine distance to the list\n",
        "        distances.append(distance)\n",
        "\n",
        "        # Store distance in the dictionary\n",
        "        sentences[i]['distance_to_next'] = distance\n",
        "\n",
        "    # Optionally handle the last sentence\n",
        "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
        "\n",
        "    return distances, sentences\n",
        "\n",
        "def extract_info(text):\n",
        "    metadata_match = re.search(r'METADATA:(.*?)\\ncategories:', text, re.DOTALL)\n",
        "    categories_match = re.search(r'categories:(.*?)\\ntags:', text, re.DOTALL)\n",
        "    tags_match = re.search(r'tags:(.*?)\\n(CONTENT|EXCERPT):', text, re.DOTALL)\n",
        "    content_match = re.search(r'(CONTENT|EXCERPT):(.*?)\\n-+', text, re.DOTALL)\n",
        "    title_match = re.search(r'TITLE:\\s*(.+)', text)\n",
        "    \n",
        "    metadata = metadata_match.group(1).strip() if metadata_match else ''\n",
        "    content_key = content_match.group(1) if content_match else 'CONTENT'\n",
        "    content = content_match.group(2).strip() if content_match else ''\n",
        "    categories = categories_match.group(1).strip() if categories_match else ''\n",
        "    tags = tags_match.group(1).strip() if tags_match else ''\n",
        "    title = title_match.group(1).strip() if title_match else 'Report...'\n",
        "\n",
        "    extracted_info = {\n",
        "        'METADATA': metadata,\n",
        "        'categories': categories,\n",
        "        'tags': tags,\n",
        "        content_key: content,\n",
        "        'TITLE': title\n",
        "    }\n",
        "    \n",
        "    return extracted_info\n",
        "\n",
        "def combine_sentences(sentences, buffer_size=1):\n",
        "    # Go through each sentence dict\n",
        "    for i in range(len(sentences)):\n",
        "\n",
        "        # Create a string that will hold the sentences which are joined\n",
        "        combined_sentence = ''\n",
        "\n",
        "        # Add sentences before the current one, based on the buffer size.\n",
        "        for j in range(i - buffer_size, i):\n",
        "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
        "            if j >= 0:\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += sentences[j]['sentence'] + ' '\n",
        "\n",
        "        # Add the current sentence\n",
        "        combined_sentence += sentences[i]['sentence']\n",
        "\n",
        "        # Add sentences after the current one, based on the buffer size\n",
        "        for j in range(i + 1, i + 1 + buffer_size):\n",
        "            # Check if the index j is within the range of the sentences list\n",
        "            if j < len(sentences):\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += ' ' + sentences[j]['sentence']\n",
        "\n",
        "        # Then add the whole thing to your dict\n",
        "        # Store the combined sentence in the current sentence dict\n",
        "        sentences[i]['combined_sentence'] = combined_sentence\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def chunk_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Splits sentences into chunks based on cosine distance between their embeddings.\n",
        "    Args:\n",
        "        sentences (list): List of sentences to be chunked\n",
        "    Returns:\n",
        "        list: List of chunked sentences\n",
        "    \"\"\"\n",
        "    sentence_embeddings = sentences_emb_model([sentence['combined_sentence'] for sentence in sentences])\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence['combined_sentence_embedding'] = sentence_embeddings[i]\n",
        "\n",
        "    distances, sentences = calculate_cosine_distances(sentences)\n",
        "\n",
        "    breakpoint_percentile_threshold = 95\n",
        "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
        "\n",
        "    # Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
        "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
        "    # Initialize the start index\n",
        "    start_index = 0\n",
        "\n",
        "    # Create a list to hold the grouped sentences\n",
        "    chunks = []\n",
        "\n",
        "    # Iterate through the breakpoints to slice the sentences\n",
        "    for index in indices_above_thresh:\n",
        "        # The end index is the current breakpoint\n",
        "        end_index = index\n",
        "\n",
        "        # Slice the sentence_dicts from the current start index to the end index\n",
        "        group = sentences[start_index:end_index + 1]\n",
        "        combined_text = ' '.join([d['sentence'] for d in group])\n",
        "        chunks.append(combined_text)\n",
        "        \n",
        "        # Update the start index for the next group\n",
        "        start_index = index + 1\n",
        "\n",
        "    # The last group, if any sentences remain\n",
        "    if start_index < len(sentences):\n",
        "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
        "        chunks.append(combined_text)\n",
        "    # Return the chunks\n",
        "    return chunks\n",
        "\n",
        "def run_anthropic_model(prompt_text, model_name = 'anthropic.claude-3-haiku-20240307-v1:0'):\n",
        "    \"\"\"\n",
        "    Run the Anthropic model with the given prompt text and model name.\n",
        "    \n",
        "    Args:\n",
        "        prompt_text (str): The prompt text to send to the model.\n",
        "        model_name (str): The name of the model to use.\n",
        "        \n",
        "    Returns:\n",
        "        str: The response from the model.\n",
        "    \"\"\"\n",
        "    # Prepare the request body for Claude 3\n",
        "    request_body = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 4096,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt_text\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.3,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 200\n",
        "    }\n",
        "\n",
        "    # Call Claude 3 model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_name,\n",
        "        body=json.dumps(request_body)\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    response_body = json.loads(response.get(\"body\").read().decode())\n",
        "    reply = response_body.get(\"content\", [])[0].get(\"text\", \"\")\n",
        "    \n",
        "    return reply\n",
        "\n",
        "def sentences_emb_model(sentences, model_id=\"cohere.embed-multilingual-v3\"):\n",
        "    \"\"\"\n",
        "    Run the embedding model on the given sentences.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): The name of the embedding model to use.\n",
        "        sentences (list): List of sentences to embed.\n",
        "        \n",
        "    Returns:\n",
        "        list: List of embeddings for the sentences.\n",
        "    \"\"\"\n",
        "    # print(len(sentences))\n",
        "    # Request payload\n",
        "    request_body = json.dumps({\n",
        "        \"texts\": [sentence[-2048:] for sentence in sentences],  # Limit to 2048 tokens\n",
        "        \"input_type\": \"search_document\"  # or \"search_query\", or \"classification\"\n",
        "    })\n",
        "    \n",
        "    # Call the model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_id,\n",
        "        body=request_body.encode(\"utf-8\"),\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\"\n",
        "    )\n",
        "\n",
        "    # Parse the response\n",
        "    response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "    return response_body[\"embeddings\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Dictionary with model names and their respective IDs on AWS Bedrock\n",
        "models = {\n",
        "    \"Claude 3 Sonnet (200k)\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "    \"Claude 3 Haiku (200k)\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "}\n",
        "chunks = []\n",
        "text = \"\"\"METADATA: {matadata}\n",
        "categories: {categories}\n",
        "tags: {tags}\n",
        "{type_key}: {content}\n",
        "TITLE: {title}\"\"\"\n",
        "num=10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count = 0\n",
        "for page_key, page_data in tqdm(pages.items(), desc=\"Processing pages\", unit=\"page\"):\n",
        "    # Skip the first 207 pages\n",
        "\n",
        "    if count == 248:\n",
        "        continue\n",
        "    # else:\n",
        "    count += 1\n",
        "    # Extract metadata, categories, tags, and content\n",
        "    extracted_info = extract_info(page_data['data'])\n",
        "\n",
        "    if 'CONTENT' in extracted_info.keys():\n",
        "        k = 'CONTENT'\n",
        "        para = extracted_info['CONTENT']\n",
        "    elif 'EXCERPT' in extracted_info.keys():\n",
        "        k = 'EXCERPT'\n",
        "        para = extracted_info['EXCERPT']\n",
        "    else:\n",
        "        raise ValueError(\"Neither CONTENT nor EXCERPT found in the extracted information.\")\n",
        "\n",
        "    sentences = re.split(r'(?<=[.?!])\\s+', para)\n",
        "    sentences = [{'sentence': sentence} for sentence in sentences if sentence]\n",
        "    # Combine sentences with a buffer size of 1\n",
        "    # print(sentences)\n",
        "    comb_sentences = combine_sentences(sentences, buffer_size=1)\n",
        "    # Add the sentences to the chunks list\n",
        "    if len(sentences) <= 1:\n",
        "        temp_text = text.format(\n",
        "            matadata=extracted_info['METADATA'],\n",
        "            categories=extracted_info['categories'],\n",
        "            tags=extracted_info['tags'],\n",
        "            type_key=k,\n",
        "            content=sentences[0]['sentence'] if len(sentences) > 0 else '',\n",
        "            title=extracted_info['TITLE'],\n",
        "        )\n",
        "#         summary_prompt = \"\"\"Summarize the given text in 1 or 2 sentences maximum, point at some key features of the text in {type_key}, TITLE, categories and tags: \n",
        "# {temp_text}\"\"\".format(\n",
        "#             type_key=k,\n",
        "#             temp_text=temp_text\n",
        "#         )\n",
        "#         # Generate the Groq response\n",
        "#         summary = run_anthropic_model(str(summary_prompt), model_name=models[\"Claude 3 Haiku (200k)\"])\n",
        "        chunk = {\n",
        "            'URL': re.search(r\"https?://[^\\s\\\"']+\", page_key).group(),\n",
        "            'text': temp_text,\n",
        "            'embedding': sentences_emb_model([text]),\n",
        "            # 'summary': summary,\n",
        "            # 'summary_embedding': sentences_emb_model([summary]),\n",
        "        }\n",
        "        chunks.append(chunk)\n",
        "        continue\n",
        "    temp_chunks = chunk_sentences(sentences)\n",
        "    for sentence in temp_chunks:\n",
        "        # Add the URL, metadata, categories, tags, type, and title to the chunk\n",
        "        temp_text = text.format(\n",
        "            matadata=extracted_info['METADATA'],\n",
        "            categories=extracted_info['categories'],\n",
        "            tags=extracted_info['tags'],\n",
        "            type_key=k,\n",
        "            content=sentence,\n",
        "            title=extracted_info['TITLE']\n",
        "        )\n",
        "#         summary_prompt = \"\"\"Summarize the given text in 1 or 2 sentences maximum, point at some key features of the text in {type_key}, TITLE, categories and tags: \n",
        "# {temp_text}\"\"\".format(\n",
        "#             type_key=k,\n",
        "#             temp_text=temp_text\n",
        "#         )\n",
        "#         # Generate the Groq response\n",
        "#         summary = run_anthropic_model(str(summary_prompt), model_name=models[\"Claude 3 Haiku (200k)\"])\n",
        "\n",
        "        chunk = {\n",
        "            'URL': re.search(r\"https?://[^\\s\\\"']+\", page_key).group(),\n",
        "            'text': temp_text,\n",
        "            'embedding': sentences_emb_model([temp_text]),\n",
        "            # 'summary': summary,\n",
        "            # 'summary_embedding': sentences_emb_model([summary]),\n",
        "        }\n",
        "        chunks.append(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "def save_pickle(data, file_path):\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "# Save the chunks to a pickle file\n",
        "save_pickle(chunks, 'chunks.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for item in chunks[:10]:\n",
        "    print(item['text'])\n",
        "    print(item['URL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load saved chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aws_access_key_id = ''\n",
        "aws_secret_access_key = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# functions\n",
        "import os\n",
        "import json\n",
        "import boto3\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Bedrock client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key\n",
        ")\n",
        "\n",
        "def calculate_cosine_distances(sentences):\n",
        "    distances = []\n",
        "    for i in range(len(sentences) - 1):\n",
        "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
        "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
        "        \n",
        "        # Convert to cosine distance\n",
        "        distance = 1 - similarity\n",
        "\n",
        "        # Append cosine distance to the list\n",
        "        distances.append(distance)\n",
        "\n",
        "        # Store distance in the dictionary\n",
        "        sentences[i]['distance_to_next'] = distance\n",
        "\n",
        "    # Optionally handle the last sentence\n",
        "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
        "\n",
        "    return distances, sentences\n",
        "\n",
        "def extract_info(text):\n",
        "    metadata_match = re.search(r'METADATA:(.*?)\\ncategories:', text, re.DOTALL)\n",
        "    categories_match = re.search(r'categories:(.*?)\\ntags:', text, re.DOTALL)\n",
        "    tags_match = re.search(r'tags:(.*?)\\n(CONTENT|EXCERPT):', text, re.DOTALL)\n",
        "    content_match = re.search(r'(CONTENT|EXCERPT):(.*?)\\n-+', text, re.DOTALL)\n",
        "    title_match = re.search(r'TITLE:\\s*(.+)', text)\n",
        "    \n",
        "    metadata = metadata_match.group(1).strip() if metadata_match else ''\n",
        "    content_key = content_match.group(1) if content_match else 'CONTENT'\n",
        "    content = content_match.group(2).strip() if content_match else ''\n",
        "    categories = categories_match.group(1).strip() if categories_match else ''\n",
        "    tags = tags_match.group(1).strip() if tags_match else ''\n",
        "    title = title_match.group(1).strip() if title_match else 'Report...'\n",
        "\n",
        "    extracted_info = {\n",
        "        'METADATA': metadata,\n",
        "        'categories': categories,\n",
        "        'tags': tags,\n",
        "        content_key: content,\n",
        "        'TITLE': title\n",
        "    }\n",
        "    \n",
        "    return extracted_info\n",
        "\n",
        "def combine_sentences(sentences, buffer_size=1):\n",
        "    # Go through each sentence dict\n",
        "    for i in range(len(sentences)):\n",
        "\n",
        "        # Create a string that will hold the sentences which are joined\n",
        "        combined_sentence = ''\n",
        "\n",
        "        # Add sentences before the current one, based on the buffer size.\n",
        "        for j in range(i - buffer_size, i):\n",
        "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
        "            if j >= 0:\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += sentences[j]['sentence'] + ' '\n",
        "\n",
        "        # Add the current sentence\n",
        "        combined_sentence += sentences[i]['sentence']\n",
        "\n",
        "        # Add sentences after the current one, based on the buffer size\n",
        "        for j in range(i + 1, i + 1 + buffer_size):\n",
        "            # Check if the index j is within the range of the sentences list\n",
        "            if j < len(sentences):\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += ' ' + sentences[j]['sentence']\n",
        "\n",
        "        # Then add the whole thing to your dict\n",
        "        # Store the combined sentence in the current sentence dict\n",
        "        sentences[i]['combined_sentence'] = combined_sentence\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def chunk_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Splits sentences into chunks based on cosine distance between their embeddings.\n",
        "    Args:\n",
        "        sentences (list): List of sentences to be chunked\n",
        "    Returns:\n",
        "        list: List of chunked sentences\n",
        "    \"\"\"\n",
        "    sentence_embeddings = sentences_emb_model([sentence['combined_sentence'] for sentence in sentences])\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence['combined_sentence_embedding'] = sentence_embeddings[i]\n",
        "\n",
        "    distances, sentences = calculate_cosine_distances(sentences)\n",
        "\n",
        "    breakpoint_percentile_threshold = 95\n",
        "    breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
        "\n",
        "    # Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
        "    indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
        "    # Initialize the start index\n",
        "    start_index = 0\n",
        "\n",
        "    # Create a list to hold the grouped sentences\n",
        "    chunks = []\n",
        "\n",
        "    # Iterate through the breakpoints to slice the sentences\n",
        "    for index in indices_above_thresh:\n",
        "        # The end index is the current breakpoint\n",
        "        end_index = index\n",
        "\n",
        "        # Slice the sentence_dicts from the current start index to the end index\n",
        "        group = sentences[start_index:end_index + 1]\n",
        "        combined_text = ' '.join([d['sentence'] for d in group])\n",
        "        chunks.append(combined_text)\n",
        "        \n",
        "        # Update the start index for the next group\n",
        "        start_index = index + 1\n",
        "\n",
        "    # The last group, if any sentences remain\n",
        "    if start_index < len(sentences):\n",
        "        combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
        "        chunks.append(combined_text)\n",
        "    # Return the chunks\n",
        "    return chunks\n",
        "\n",
        "def run_anthropic_model(prompt_text, model_name = 'anthropic.claude-3-haiku-20240307-v1:0'):\n",
        "    \"\"\"\n",
        "    Run the Anthropic model with the given prompt text and model name.\n",
        "    \n",
        "    Args:\n",
        "        prompt_text (str): The prompt text to send to the model.\n",
        "        model_name (str): The name of the model to use.\n",
        "        \n",
        "    Returns:\n",
        "        str: The response from the model.\n",
        "    \"\"\"\n",
        "    # Prepare the request body for Claude 3\n",
        "    request_body = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 4096,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt_text\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.3,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 200\n",
        "    }\n",
        "\n",
        "    # Call Claude 3 model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_name,\n",
        "        body=json.dumps(request_body)\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    response_body = json.loads(response.get(\"body\").read().decode())\n",
        "    reply = response_body.get(\"content\", [])[0].get(\"text\", \"\")\n",
        "\n",
        "    return reply\n",
        "\n",
        "def sentences_emb_model(sentences, model_id=\"cohere.embed-multilingual-v3\"):\n",
        "    \"\"\"\n",
        "    Run the embedding model on the given sentences.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): The name of the embedding model to use.\n",
        "        sentences (list): List of sentences to embed.\n",
        "        \n",
        "    Returns:\n",
        "        list: List of embeddings for the sentences.\n",
        "    \"\"\"\n",
        "    # print(len(sentences))\n",
        "    # Request payload\n",
        "    request_body = json.dumps({\n",
        "        \"texts\": [sentence[-2048:] for sentence in sentences],  # Limit to 2048 tokens\n",
        "        \"input_type\": \"search_document\"  # or \"search_query\", or \"classification\"\n",
        "    })\n",
        "    \n",
        "    # Call the model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=model_id,\n",
        "        body=request_body.encode(\"utf-8\"),\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\"\n",
        "    )\n",
        "\n",
        "    # Parse the response\n",
        "    response_body = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "    return response_body[\"embeddings\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def load_pickle(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data\n",
        "\n",
        "chunks = load_pickle('chunks.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "\n",
        "def load_vector_db(vector_db_path='chunks.pkl'):\n",
        "    \"\"\"\n",
        "    Load the FAISS index and metadata from the vector database.\n",
        "    \n",
        "    Args:\n",
        "        vector_db_path: Path to the directory containing the vector database\n",
        "        \n",
        "    Returns:\n",
        "        vector_db: chunked vector dataset\n",
        "    \"\"\"\n",
        "    return load_pickle(vector_db_path)\n",
        "\n",
        "def search(query_embedding, chunk_embeddings, k=5):\n",
        "    \"\"\"\n",
        "    Perform a search in the vector database using the query embedding.\n",
        "    \n",
        "    Args:\n",
        "        query_embedding: Embedding of the query\n",
        "        chunk_embeddings: Embeddings of the chunks in the database\n",
        "        k: Number of top results to return\n",
        "        \n",
        "    Returns:\n",
        "        distances: Distances of the top k results\n",
        "        indices: Indices of the top k results\n",
        "    \"\"\"\n",
        "    # Calculate cosine similarity\n",
        "    distances = []\n",
        "    for i in range(len(chunk_embeddings)):\n",
        "        embedding_current = chunk_embeddings[i]\n",
        "        \n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity(query_embedding, embedding_current)[0][0]\n",
        "        \n",
        "        # Convert to cosine distance\n",
        "        distance = 1 - similarity\n",
        "\n",
        "        # Append cosine distance to the list\n",
        "        distances.append(distance)\n",
        "    # Get top k results\n",
        "    indices = np.argsort(distances)[:k]\n",
        "    \n",
        "    return distances, indices\n",
        "\n",
        "def search_vector_db(query, vector_db_info, top_k=5, rerank=True):\n",
        "    \"\"\"\n",
        "    Search the vector database for the most similar chunks to the query.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text\n",
        "        top_k: Number of top results to return\n",
        "        vector_db_path: Path to the vector database directory\n",
        "        rerank: Whether to rerank results using more sophisticated similarity\n",
        "        \n",
        "    Returns:\n",
        "        List of dictionaries containing chunk info and similarity scores\n",
        "    \"\"\"\n",
        "    # Load the vector database\n",
        "    chunk_texts = [chunk['text'] for chunk in vector_db_info]\n",
        "    chunk_txt_emb = [chunk['embedding'] for chunk in vector_db_info]\n",
        "    chunk_urls = [chunk['URL'] for chunk in vector_db_info]\n",
        "\n",
        "    # Encode the query\n",
        "    query_embedding = sentences_emb_model([query])\n",
        "\n",
        "    # Search the index - retrieve more results than needed for reranking\n",
        "    k_search = top_k * 3 if rerank else top_k\n",
        "    k_search = min(k_search, len(chunk_texts))  # Don't try to get more results than we have chunks\n",
        "\n",
        "    distances, indices = search(query_embedding, chunk_txt_emb, k_search)\n",
        "\n",
        "    # Process results\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices):\n",
        "        if idx != -1:  # Valid index\n",
        "            chunk_text = chunk_texts[idx]\n",
        "            similarity = float(distances[i])\n",
        "            results.append({\n",
        "                \"text\": chunk_text,\n",
        "                \"url\": chunk_urls[idx],\n",
        "                \"similarity\": similarity\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_surrounding_context(result, metadata, context_chunks=1):\n",
        "    \"\"\"\n",
        "    Get surrounding chunks as context for a result.\n",
        "    \n",
        "    Args:\n",
        "        result: A single result dictionary from search_vector_db\n",
        "        metadata: Metadata from the vector database\n",
        "        context_chunks: Number of chunks to include before and after\n",
        "        \n",
        "    Returns:\n",
        "        String with the context including the result chunk\n",
        "    \"\"\"\n",
        "    chunks = metadata[\"chunks\"]\n",
        "    chunk_metadata = metadata[\"chunk_metadata\"]\n",
        "    \n",
        "    file_path = result[\"file_path\"]\n",
        "    chunk_index = result[\"chunk_index\"]\n",
        "    \n",
        "    # Find chunks from the same file\n",
        "    same_file_chunks = []\n",
        "    chunk_indices = []\n",
        "    \n",
        "    for i, info in enumerate(chunk_metadata):\n",
        "        if info[\"file_path\"] == file_path:\n",
        "            same_file_chunks.append(chunks[i])\n",
        "            chunk_indices.append(info[\"chunk_index\"])\n",
        "    \n",
        "    # Sort by chunk index\n",
        "    sorted_chunks = [x for _, x in sorted(zip(chunk_indices, same_file_chunks))]\n",
        "    \n",
        "    # Find the index of our chunk in the sorted list\n",
        "    file_chunk_index = chunk_indices.index(chunk_index)\n",
        "    \n",
        "    # Get context chunks\n",
        "    start_idx = max(0, file_chunk_index - context_chunks)\n",
        "    end_idx = min(len(sorted_chunks), file_chunk_index + context_chunks + 1)\n",
        "    \n",
        "    context_text = \"\\n\\n\".join(sorted_chunks[start_idx:end_idx])\n",
        "    return context_text\n",
        "\n",
        "def retrive(query, vector_db_info, top_k=3):\n",
        "    \"\"\"\n",
        "    Perform retrival for specific the query for \"What is attention?\" and format the results.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text \n",
        "        top_k: Number of top results to return\n",
        "        vector_db_path: Path to the vector database directory\n",
        "        \n",
        "    Returns:\n",
        "        None, prints formatted results\n",
        "    \"\"\"\n",
        "    # print(f\"Searching for: '{query}'\")\n",
        "    \n",
        "    # Get search results\n",
        "    results = search_vector_db(query, vector_db_info, top_k=top_k)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def date_time_stamp(format = \"%m%d%H%M%S\"):\n",
        "    return datetime.now().strftime(format)\n",
        "\n",
        "def extract_json(text):\n",
        "    \"\"\"\n",
        "    Detect and extract JSON content from a given text.\n",
        "    Supports cases where JSON is wrapped inside additional text.\n",
        "    \"\"\"\n",
        "    # Try to extract JSON array from text (Variation Two)\n",
        "    json_match = re.search(r\"\\[\\s*{.*?}\\s*\\]\", text, re.DOTALL)\n",
        "    \n",
        "    if json_match:\n",
        "        json_str = json_match.group(0)\n",
        "        try:\n",
        "            parsed_json = json.loads(json_str)\n",
        "            return parsed_json\n",
        "        except json.JSONDecodeError:\n",
        "            return None  # Invalid JSON format inside text\n",
        "    \n",
        "    return None  # No JSON found\n",
        "\n",
        "def remove_think_section(text, tag='think'):\n",
        "    if not text:\n",
        "        return text\n",
        "    elif tag == 'think':\n",
        "        return re.sub(r\"<think>.*?</think>\\s*\", \"\", text, flags=re.DOTALL)\n",
        "    elif tag == 'response':\n",
        "        return re.sub(r\"<response>.*?</response>\\s*\", \"\", text, flags=re.DOTALL)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown tag: {tag}\")\n",
        "\n",
        "def print_urls_nicely(urls):\n",
        "    \"\"\"\n",
        "    Prints extracted URLs in a clean and visually appealing manner using print().\n",
        "    \"\"\"\n",
        "    print(\"\\n=== References ===\\n\")\n",
        "    for i, url in enumerate(urls):\n",
        "        print(f\"{i+1}. ðŸ“Œ {url}\\n\")\n",
        "\n",
        "def extract_url_and_context(documents):\n",
        "    \"\"\"\n",
        "    Extracts URLs and relevant context from retrieved documents.\n",
        "    Cleans up the content by removing metadata and irrelevant tags.\n",
        "    \"\"\"\n",
        "    extracted_data = []\n",
        "    context_text = \"\"\n",
        "    for i, doc in enumerate(documents):\n",
        "        url = doc.get(\"url\", \"No URL Provided\")\n",
        "        \n",
        "        # Extracting content from text, ignoring metadata\n",
        "        text = doc.get(\"text\", \"\")\n",
        "        cleaned_text = text.split(\"CONTENT:\")[-1].strip()  # Extract content after \"CONTENT:\"\n",
        "        context_text += f\"Information chunk {i+1}:\\n\" + cleaned_text + \"\\n\\n\"\n",
        "        \n",
        "        if not url in extracted_data:\n",
        "            extracted_data.append(url)\n",
        "\n",
        "    return extracted_data, context_text\n",
        "\n",
        "def remove_response_tags(text):\n",
        "    \"\"\"\n",
        "    Removes <response> tags and extracts the clean text.\n",
        "    \"\"\"\n",
        "    cleaned_text = re.sub(r\"</?response>\", \"\", text).strip()\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class RAG_chatbot:\n",
        "    def __init__(self, api_key: str = None):\n",
        "        self.history = ''\n",
        "        # self.doc_gen_model = 'gemma2-9b-it'\n",
        "        self.doc_gen_model = 'llama-3.1-8b-instant'\n",
        "        self.s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key\n",
        "        )\n",
        "        # Dictionary with model names and their respective IDs on AWS Bedrock\n",
        "        models = {\n",
        "            \"Claude 3 Sonnet (200k)\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "            \"Claude 3 Haiku (200k)\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "        }\n",
        "        self.gen_model = models['Claude 3 Haiku (200k)']\n",
        "\n",
        "        # Initialize Bedrock client\n",
        "        self.bedrock_client = boto3.client(\n",
        "            service_name='bedrock-runtime',\n",
        "            region_name='us-east-1',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key\n",
        "        )\n",
        "\n",
        "        self.vector_db_info = load_vector_db()\n",
        "\n",
        "        with open(\"prompt.txt\", \"r\") as file:\n",
        "            self.chat_prompt = file.read()\n",
        "        \n",
        "        with open(\"prompt_gen.txt\", \"r\") as file:\n",
        "            self.gen_prompt = file.read()\n",
        "        \n",
        "    def query(self, query: str):\n",
        "        \n",
        "        s_time = date_time_stamp('%H:%M:%S')\n",
        "        start_time = time.time()  # Record the start time\n",
        "\n",
        "\n",
        "\n",
        "        # RETRIVAL PIPE-LINE\n",
        "        # generate duplicate questions\n",
        "\n",
        "        # Load prompt for generation\n",
        "        message = self.gen_prompt.format(question=query)\n",
        "\n",
        "        # generate questions using the model in Json format\n",
        "        generate_doc = run_anthropic_model(message)\n",
        "        # extract the questions from the response\n",
        "        questions = extract_json(generate_doc)\n",
        "        results = []\n",
        "        # retrival for user query\n",
        "        results.append(retrive(query, self.vector_db_info, top_k=3)[0])\n",
        "        for question in questions:\n",
        "            # retrive using generated questions\n",
        "            Q = question['Question']+'\\n' + question['Key_features']\n",
        "            result = retrive(Q, self.vector_db_info, top_k=3)\n",
        "            if result:\n",
        "                results.append(result[0])\n",
        "        # Extract URLs and context from the retrieved documents\n",
        "        urls, context = extract_url_and_context(results[:3])\n",
        "\n",
        "\n",
        "\n",
        "        # GENERATE RESPONSE using the retrieved document\n",
        "        message = [\n",
        "            {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": self.chat_prompt.format(history=self.history, query=query, context=context)\n",
        "            }\n",
        "        ]\n",
        "        request_body = {\n",
        "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "            \"max_tokens\": 4096,\n",
        "            \"messages\": message,\n",
        "            \"temperature\": 0.3,\n",
        "            \"top_p\": 0.8,\n",
        "            \"top_k\": 200\n",
        "        }\n",
        "\n",
        "        # Call Claude 3 model\n",
        "        response = self.bedrock_client.invoke_model(\n",
        "            modelId=self.gen_model,\n",
        "            body=json.dumps(request_body)\n",
        "        )\n",
        "        response_body = json.loads(response.get(\"body\").read().decode())\n",
        "        reply = (response_body.get(\"content\", [])[0].get(\"text\", \"\"))\n",
        "        reply = remove_response_tags(reply)\n",
        "\n",
        "        latency = time.time() - start_time  # Calculate latency\n",
        "\n",
        "        self.json_history.append(\n",
        "            {\n",
        "                'query': query,\n",
        "                'context': context,\n",
        "                'response': reply,\n",
        "                'latency': latency,\n",
        "                'start_time': s_time,\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        with open(self.file_path, 'w') as f:\n",
        "            json.dump(self.json_history, f, indent=4)\n",
        "        \n",
        "        self.history += '\\n' + 'You: ' + query + '\\n' + 'Motiv-ator: ' + reply\n",
        "        print('\\n' + 'You: ' + query + '\\n' + 'Motiv-ator: ' + reply)\n",
        "        if urls:\n",
        "            print_urls_nicely(urls)\n",
        "        return\n",
        "    \n",
        "\n",
        "    def Start_conversation(self):\n",
        "        \"\"\"\n",
        "        Start a conversation with the chatbot.\n",
        "        Args:\n",
        "            save_trace: Whether to save the trace data\n",
        "        \"\"\"\n",
        "        # create directory for json files\n",
        "        time_stamp = date_time_stamp()\n",
        "        file_path = time_stamp+'.json'\n",
        "        self.json_history = []\n",
        "        self.file_path = file_path\n",
        "        with open(\"Chat_greeting.txt\", \"r\") as file:\n",
        "            print(file.read())\n",
        "        count = 0\n",
        "        while True:\n",
        "            count+=1\n",
        "            sys.stdout.flush()\n",
        "            query = input(\"You: \")\n",
        "            if not query or query.lower() == \"exit\" or query.lower() == \"quit\":\n",
        "                break\n",
        "            _ = self.query(query)\n",
        "        print(\"Thank you for your time and Goodbye :)\")\n",
        "        return self.json_history\n",
        "\n",
        "bot = RAG_chatbot()\n",
        "# b = bot.query(\"why motive over lynx?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_history = bot.Start_conversation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqGV6-K9raAV"
      },
      "source": [
        "### Problem 2: Video-Text Index\n",
        "####Design a system that enables a set of videos to be searchable.\n",
        "\n",
        "You are provided a dataset of 500 videos with which to build and test your solution. The 500 videos are comprised of the following categories\n",
        "* 50 collision videos\n",
        "* 50 tailgating videos\n",
        "* 50 stop sign violation videos\n",
        "* 50 red light violation videos\n",
        "* 300 random videos correponding to hard brakes, corners, and accelerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMzuni4DoLbq",
        "outputId": "52f00563-5a64-4a6a-b08a-689760de9213"
      },
      "outputs": [],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKah9L5rWWg"
      },
      "source": [
        "#### Sample snippets to access the videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe8dp-uGmXhH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import boto3\n",
        "import io\n",
        "\n",
        "def read_s3_csv_to_dataframe(bucket_name, key_name, aws_access_key_id, aws_secret_access_key):\n",
        "    \"\"\"Reads a CSV file from S3 into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The S3 bucket name.\n",
        "        key_name (str): The S3 object key (file name).\n",
        "        aws_access_key_id (str): Your AWS access key ID.\n",
        "        aws_secret_access_key (str): Your AWS secret access key.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame containing the CSV data, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "        obj = s3.get_object(Bucket=bucket_name, Key=key_name)\n",
        "        df = pd.read_csv(io.BytesIO(obj['Body'].read()), sep='\\t')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV from S3: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "bucket_name = 'motiverse-2025-data'  # Replace with your bucket name\n",
        "key_name = 'video_index.tsv'  # Replace with your CSV file name in S3\n",
        "df = read_s3_csv_to_dataframe(bucket_name, key_name, aws_access_key_id, aws_secret_access_key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "adhYcTqToTFq",
        "outputId": "71ec443c-3459-4e30-bb42-cdda2d6964ec"
      },
      "outputs": [],
      "source": [
        "df.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed-zThP8nbeT"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import tempfile\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "from base64 import b64encode\n",
        "\n",
        "def render_s3_video(bucket_name, key_name, width=640):\n",
        "    \"\"\"\n",
        "    Downloads an MP4 file from S3 and renders it in a Colab notebook.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The S3 bucket name\n",
        "        key_name (str): The S3 object key (path to the MP4 file)\n",
        "        width (int): Width to display the video (in pixels)\n",
        "    \"\"\"\n",
        "    # Create S3 client - this will use credentials from environment or instance profile\n",
        "    # Pass credentials as keyword arguments\n",
        "    s3_client = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
        "\n",
        "    # Create a temporary file to store the video\n",
        "    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "        temp_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        # Download the file from S3\n",
        "        print(f\"Downloading video from s3://{bucket_name}/{key_name}...\")\n",
        "        s3_client.download_file(bucket_name, key_name, temp_path)\n",
        "\n",
        "        # Read the file content\n",
        "        with open(temp_path, 'rb') as f:\n",
        "            video_data = f.read()\n",
        "\n",
        "        # Convert to base64\n",
        "        video_base64 = b64encode(video_data).decode('utf-8')\n",
        "\n",
        "        # Display the video\n",
        "        video_html = f\"\"\"\n",
        "        <video width=\"{width}\" controls>\n",
        "          <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "          Your browser does not support the video tag.\n",
        "        </video>\n",
        "        \"\"\"\n",
        "\n",
        "        display(HTML(video_html))\n",
        "        print(f\"Video displayed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temp file\n",
        "        if os.path.exists(temp_path):\n",
        "            os.remove(temp_path)\n",
        "\n",
        "# Example usage\n",
        "# render_s3_video('my-bucket', 'path/to/video.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "w8VlmkL-nbg_",
        "outputId": "5fd6b7f4-576f-4652-e8a1-77c1781a1bed"
      },
      "outputs": [],
      "source": [
        "render_s3_video('motiverse-2025-data', 'videos/3043711272.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4GMsRzqowAM"
      },
      "source": [
        "#### List of foundational LLMs aviailable from Amazon BedRock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NVQULtsnSWa",
        "outputId": "0ef7ca05-2400-4a23-df01-76bb645af465"
      },
      "outputs": [],
      "source": [
        "# Initialize the Bedrock client\n",
        "bedrock_client = boto3.client(\n",
        "    service_name='bedrock',\n",
        "    region_name='us-east-1',  # Adjust region as needed\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key\n",
        ")\n",
        "\n",
        "def list_foundation_models():\n",
        "    \"\"\"Lists available foundation models in Bedrock.\"\"\"\n",
        "    try:\n",
        "        response = bedrock_client.list_foundation_models()\n",
        "        models = response.get('modelSummaries', [])\n",
        "\n",
        "        if models:\n",
        "            print(\"Available Foundation Models:\")\n",
        "            for model in models:\n",
        "                print(f\" - {model['modelId']}: {model['modelArn']}\")\n",
        "        else:\n",
        "            print(\"No foundation models found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "list_foundation_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvzCNF_7pFio"
      },
      "source": [
        "#### Sample LLM call with video and prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GO4zRIrpElz",
        "outputId": "08364d81-e03e-4ad6-c6f8-a8d307db2c5c"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import json\n",
        "import cv2\n",
        "import base64\n",
        "import os\n",
        "import tempfile\n",
        "from datetime import datetime\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def encode_image_to_base64(frame):\n",
        "    \"\"\"Convert OpenCV frame to base64 string.\"\"\"\n",
        "    _, buffer = cv2.imencode('.jpg', frame)\n",
        "    return base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "def process_frames_batch(bedrock_client, frames, prompt):\n",
        "    \"\"\"Process a batch of frames using Claude 3.\"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": prompt\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Add frames to the messages\n",
        "    for frame in frames:\n",
        "        base64_image = encode_image_to_base64(frame)\n",
        "        messages[0][\"content\"].append({\n",
        "            \"type\": \"image\",\n",
        "            \"source\": {\n",
        "                \"type\": \"base64\",\n",
        "                \"media_type\": \"image/jpeg\",\n",
        "                \"data\": base64_image\n",
        "            }\n",
        "        })\n",
        "\n",
        "    # Prepare the request body for Claude 3\n",
        "    request_body = {\n",
        "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
        "        \"max_tokens\": 4096,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": 0.3,\n",
        "        \"top_p\": 0.8,\n",
        "        \"top_k\": 200\n",
        "    }\n",
        "\n",
        "    # Call Claude 3 model\n",
        "    response = bedrock_client.invoke_model(\n",
        "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
        "        body=json.dumps(request_body)\n",
        "    )\n",
        "\n",
        "    # Process the response\n",
        "    response_body = json.loads(response.get(\"body\").read().decode())\n",
        "    return response_body.get(\"content\", [])[0].get(\"text\", \"\")\n",
        "\n",
        "def process_s3_video(bucket_name, key_name, aws_access_key_id, aws_secret_access_key, width=640, batch_size=5):\n",
        "    \"\"\"\n",
        "    Downloads a video from S3, processes it for snow detection, and displays it.\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): The S3 bucket name\n",
        "        key_name (str): The S3 object key (path to the MP4 file)\n",
        "        aws_access_key_id (str): AWS access key ID\n",
        "        aws_secret_access_key (str): AWS secret access key\n",
        "        width (int): Width to display the video (in pixels)\n",
        "        batch_size (int): Number of frames to process in each batch\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize S3 client\n",
        "        s3_client = boto3.client(\n",
        "            's3',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key\n",
        "        )\n",
        "\n",
        "        # Initialize Bedrock client\n",
        "        bedrock_client = boto3.client(\n",
        "            service_name='bedrock-runtime',\n",
        "            region_name='us-east-1',\n",
        "            aws_access_key_id=aws_access_key_id,\n",
        "            aws_secret_access_key=aws_secret_access_key\n",
        "        )\n",
        "\n",
        "        # Create temporary file for video\n",
        "        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_file:\n",
        "            temp_path = temp_file.name\n",
        "\n",
        "        try:\n",
        "            # Download video from S3\n",
        "            s3_client.download_file(bucket_name, key_name, temp_path)\n",
        "\n",
        "            # Display video\n",
        "            with open(temp_path, 'rb') as f:\n",
        "                video_data = f.read()\n",
        "            video_base64 = base64.b64encode(video_data).decode('utf-8')\n",
        "            video_html = f\"\"\"\n",
        "            <video width=\"{width}\" controls>\n",
        "              <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "              Your browser does not support the video tag.\n",
        "            </video>\n",
        "            \"\"\"\n",
        "            display(HTML(video_html))\n",
        "\n",
        "            # Process video for snow detection\n",
        "            video = cv2.VideoCapture(temp_path)\n",
        "            if not video.isOpened():\n",
        "                raise Exception(f\"Could not open video file: {temp_path}\")\n",
        "\n",
        "            # Extract frames (every 3rd frame for better snow detection)\n",
        "            frames = []\n",
        "            frame_count = 0\n",
        "            while video.isOpened():\n",
        "                ret, frame = video.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                if frame_count % 3 == 0:  # Extract every 3rd frame for more frequent sampling\n",
        "                    frames.append(frame)\n",
        "                frame_count += 1\n",
        "            video.release()\n",
        "\n",
        "            # Prepare the prompt for snow detection\n",
        "            prompt = \"\"\"Analyze the following video frames and determine if there is snow present.\n",
        "            Look for these specific indicators of snow:\n",
        "            1. White or light-colored ground cover\n",
        "            2. Snowflakes falling in the air\n",
        "            3. Snow accumulation on surfaces\n",
        "            4. People or objects covered in snow\n",
        "            5. Snow-related activities (snowball fights, skiing, etc.)\n",
        "\n",
        "            For each frame, provide:\n",
        "            - Whether snow is present (Yes/No)\n",
        "            - Confidence level (High/Medium/Low)\n",
        "            - Specific evidence of snow if present\n",
        "\n",
        "            Finally, provide an overall assessment:\n",
        "            - Is snow present in the video? (Yes/No)\n",
        "            - What percentage of frames show snow?\n",
        "            - Any notable changes in snow conditions throughout the video?\"\"\"\n",
        "\n",
        "            # Process frames in batches\n",
        "            all_analyses = []\n",
        "            for i in range(0, len(frames), batch_size):\n",
        "                batch = frames[i:i + batch_size]\n",
        "                print(f\"Processing batch {i//batch_size + 1} of {(len(frames) + batch_size - 1)//batch_size}\")\n",
        "                batch_analysis = process_frames_batch(bedrock_client, batch, prompt)\n",
        "                all_analyses.append(batch_analysis)\n",
        "\n",
        "            # Combine all analyses\n",
        "            combined_analysis = \"\\n\\n\".join(all_analyses)\n",
        "\n",
        "            # Save the analysis to a file\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_file = f\"snow_analysis_{timestamp}.txt\"\n",
        "            with open(output_file, \"w\") as f:\n",
        "                f.write(f\"Snow Analysis for: {bucket_name}/{key_name}\\n\")\n",
        "                f.write(f\"Total Frames Analyzed: {len(frames)}\\n\")\n",
        "                f.write(f\"Analysis:\\n{combined_analysis}\\n\")\n",
        "\n",
        "            print(f\"Snow analysis complete! Results saved to {output_file}\")\n",
        "            return combined_analysis\n",
        "\n",
        "        finally:\n",
        "            # Clean up temporary file\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "process_s3_video(\n",
        "    bucket_name='motiverse-2025-data',\n",
        "    key_name='videos/3043711272.mp4',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        "    batch_size=5  # Process 5 frames at a time\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp-aditya",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
